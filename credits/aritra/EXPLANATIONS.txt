the code for seac implementations uploaded by arighosh05 involved AI assistance. the high-level idea behind the seac implementations was a shared experience actor-critic approach. each agent (or warehouse robot) was an actor with its own policy network, and there exists a global critic with a value network. both these networks are learned during training. the core extension of SEAC is the shared experience aspect, which is merely that each agent has access to a global buffer that stores the experiences of all the agents. the agents then update their policy network based on the overall updates. this is what happens at a high level in all SEAC implementations. a fine point worth mentioning is that soem form of reward shaping is included, where agents moving towards the drop-off location (measured via Manhattan distance) are granted a bonus.  
